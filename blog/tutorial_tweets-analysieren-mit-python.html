<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
    <title>Sophie Schneider</title>
  </head>
  <body>
    <div class="description">
      <h1>Tutorial: Tweets analysieren mit Python</h1>
      <p>Published November 3, 2020 by Sophie Schneider</p>
      <a href="../index.html" class="internal-link"><button type="button" class="not-active btn btn-light btn-back" data-toggle="button" aria-pressed="false" autocomplete="off">
        Back to Home Page
      </button></a>
    </div>
      <div class="container">
        
    Dieses Tutorial basiert grundsätzlich auf einer Hausarbeit im Modul "Datenanalyse &amp; -auswertung", welches ich im vergangenen Semester im Rahmen meines Studiums <a class="external-link" href="https://www.ibi.hu-berlin.de/de/studium/studiengaenge/master" target="_blank" rel="noopener noreferrer">"Information Science"</a> an der HU Berlin belegt habe. Auch wenn es online bereits zahlreiche vergleichbare Ressourcen gibt, habe ich mich schlussendlich doch für eine Veröffentlichung entschieden, nicht zuletzt da mir selbst in der Vergangenheit (u.a. auch für die genannte Hausarbeit) entsprechende Beispiele und Erklärungen schon häufig weitergeholfen haben.

<h2><span id="inhalt">Inhalt</span></h2>
<ol style="list-style-type: upper-roman;">
<li>Einleitung <a class="internal-link" href="#einleitung">↓</a></li>
<li>Vorbereitung <a class="internal-link" href="#vorbereitung">↓</a></li>
<li>Preprocessing <a class="internal-link" href="#preprocessing">↓</a></li>
<li>Textverarbeitung <a class="internal-link" href="#textverarbeitung">↓</a></li>
<li>Datenanalyse <a class="internal-link" href="#analyse">↓</a></li>
  <ol style="list-style-type: lower-alpha;">
    <li>Deskriptive Analyse <a class="internal-link" href="#deskriptive-analyse">↓</a></li>
    <li>Relationen zwischen Variablen <a class="internal-link" href="#relationen">↓</a></li>
    <li>Weiterführende Analyse <a class="internal-link" href="#weiterführende-analyse">↓</a></li>
  </ol>
<li>Ausblick <a class="internal-link" href="#ausblick">↓</a></li>
</ol>

<h2><span id="einleitung">Einleitung</span></h2>

Wir befinden uns in einer Zeit, in der nicht nur Verschwörungstheorien vom "Informationskrieg" handeln, sondern auch die obersten Ränge von Politik, Wissenschaft und Gesellschaft die Gefahren rund um <b>Desinformation und "Fake News"</b> erkennen und Lösungsansätze entwickeln - eine Thematik also, die gerade für die Informationswissenschaft von heute eine wesentliche Rolle spielt. Diese hochaktuelle Debatte soll u.a. Gegenstand des vorliegenden Tutorials sein und als Motivator dient folgender Tweet Donald Trumps vom 11. Dezember 2014:
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en">Policy towards our enemies: Hit them hard, hit them fast, hit them often &amp; then tell them it was because they are the enemy!</p>
— Donald J. Trump (@realDonaldTrump) <a href="https://twitter.com/realDonaldTrump/status/543118181846761473?ref_src=twsrc%5Etfw">December 11, 2014</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Dass Trump später immer wieder die primär nicht-konservativen Nachrichtenmedien als Feind der amerikanischen Bevölkerung bezeichnet, wird unter anderem in dem Artikel "Defining the Enemy: How Donald Trump Frames the News Media" [<a class="internal-link" href="#meeks-2019">Meeks 2019</a>] herausgestellt. Allgemein sind <b>Nachrichten und Medien</b> ein vergleichsweise häufig adressiertes Thema in Tweets Trumps [<a class="internal-link" href="#meeks-2019">Meeks 2019</a>, S. 17] [<a class="internal-link" href="#wang-2016">Wang et al. 2016</a>, S. 721], dessen Analyse relevante und neuartige Ergebnisse hervorbringen kann. Unser Mediensystem vollzieht aktuell eine Transformation von kontrollierten Top-Down-Prozessen mit journalistischen Gatekeepern hin zu einem stärker dezentralisierten <b>"hybrid media system"</b> (der Begriff geht auf Chadwicks "The Hybrid Media System: Politics and Power" zurück), in welchem digitale wie gedruckte Medien koexistieren. Die sozialen Medien erleichtern hierbei nicht nur die direkte Ansprache der Zielgruppe sowie die Dissemination von Information an ein potentiell nahezu unbegrenztes Publikum, sie unterstützen vor allem die "Entwicklung populistischer, ethnonationaler und Anti-Establishment Communities" [frei übersetzt nach <a class="internal-link" href="#wells-2020">Wells et al. 2020</a>, S. 663] in besonderem Maße.

Weiterhin verwendet Trump sogenanntes <b>"framing"</b> als Überzeugungsstrategie: "the presence or absence of certain keywords, stock phrases, stereotyped images, sources of information, and sentences that provide thematically reinforcing clusters of facts or judgments" [<a class="internal-link" href="#entman-1993">Entman 1993</a>, S. 52, zitiert nach <a class="internal-link" href="#meeks-2019">Meeks (2019)</a>, S. 3]. Twitters Retweet-Funktion unterstützt das Framing zusätzlich:
<blockquote><span class="citation">Twitter gives users an easy way to repeat Trump's frames verbatim via the retweet function. [...] Twitter's 'shareability' enables Trump's frames and influence to spread outward across peer networks, adding momentum to his framing.</span> (<a class="external-link" href="#meeks-2019">Meeks 2019</a>, S. 7)</blockquote>

Insofern liegt die Verantwortung für Inhalte nicht mehr ausschließlich beim Informationsproduzenten, sondern auch bei der Plattform, welche die vielfältigen Funktionalitäten zur Vernetzung und Informationsdiffusion zwischen Akteuren ermöglicht. <a class="internal-link" href="#inhalt">↑</a>

<h2><span id="vorbereitung">Vorbereitung</span></h2>
In diesem Tutorial wird die Programmiersprache <a class="external-link" href="https://www.python.org/downloads/" target="_blank" rel="noopener noreferrer">Python</a> verwendet. Die Codebeispiele stammen aus einem <a class="external-link" href="https://jupyter.org/install" target="_blank" rel="noopener noreferrer">Jupyter Notebook</a>, welches über <a class="external-link" href="https://github.com/BibWiss/tutorial_python-tweet-analysis" target="_blank" rel="noopener noreferrer">Github</a> abgerufen werden kann.  

Für die Analyse der Twitterdaten wurde eine ganze Reihe an Python-Bibliotheken verwendet, u.a. zur Formatierung und Verarbeitung der Daten sowie für Statistiken und Visualisierungen. In einem ersten Schritt müssen diese Bibliotheken mit einem package manager wie pip oder Conda installiert werden, bevor sie importiert werden können. 

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>#import all necessary modules

# general
import json
import pandas as pd

#numbers and time
import numpy as np
from collections import Counter
from datetime import datetime

# text
import re #regular expressions
import preprocessor as p #from tweet-preprocessor library, see https://pypi.org/project/tweet-preprocessor/
from textblob import TextBlob #text processing library, here used to extract sentiment, see also https://textblob.readthedocs.io/en/dev/
from nltk import ngrams

#stats
from scipy import stats
import researchpy as rp

# visualizations
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
from plotly.colors import n_colors
import seaborn as sn
</code></pre>
</div>

Der hier verwendete Datensatz wurde über das <a class="external-link" href="https://www.thetrumparchive.com/" target="_blank" rel="noopener noreferrer">Trump Twitter Archive</a> abgerufen. Diese Sammlung enthält bereits jetzt über 50.000 Tweets des persönlichen Twitter-Accounts Donald Trumps <a class="external-link" href="https://twitter.com/realdonaldtrump" target="_blank" rel="noopener noreferrer">@realDonaldTrump</a> und eine Weiterverwendung dieser Daten empfiehlt sich, da im Gegensatz zur Twitter API (siehe hierzu <a class="external-link" href="https://developer.twitter.com/en/docs/twitter-api/v1/tweets/post-and-engage/guides/tweet-availability" target="_blank" rel="noopener noreferrer">Tweet Availability</a>) auch die Mehrheit der gelöschten Tweets zugänglich gemacht werden (interessant ist in diesem Zusammenhang auch der Blogpost <a class="external-link" href="https://medium.com/on-archivy/tweets-and-deletes-727ed74f84ed" target="_blank" rel="noopener noreferrer">Tweets and Deletes</a>). Der Datensatz wurde am 04.08.2020 über eine Suche nach folgenden Begriffen (mit der Option "exact word search", Groß-/Kleinschreibung ausgeschlossen) generiert:
<ul>
<li>news</li>
<li>media</li>
<li>press</li>
<li>fact</li>
<li>facts</li>
<li>information</li>
<li>cnn</li>
<li>nbc</li>
<li>abc</li>
<li>cbs</li>
<li>nytimes</li>
<li>newyorktimes</li>
<li>ny times</li>
<li>fox</li>
</ul> 
<a class="internal-link" href="#inhalt">↑</a>
 
<h2><span id="preprocessing">Preprocessing</span></h2>

Im Anschluss geht es an die Vorbereitung und Bereinigung der Daten, wobei zusätzliche Spalten aus den Inhalten der bisherigen Daten erzeugt und auch durch weiterführende Analysen neue Spalten hinzufügt werden können. Als Erstes müssen jedoch die Daten, welche als JSON-Datei vorliegen, geladen und in einen <strong>Pandas Dataframe konvertiert</strong> werden. Dies erleichtert spätere Analysen.   

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>#read json file

with open("dataset_media.json", 'r',encoding="utf8") as f:
        datastore = json.load(f)

# convert json to dataframe

df = pd.json_normalize(datastore)
df
</code></pre></div>

<table>
  <thead>
    <tr>
      <th></th>
      <th>source</th>
      <th>text</th>
      <th>created_at</th>
      <th>retweet_count</th>
      <th>favorite_count</th>
      <th>is_retweet</th>
      <th>id_str</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Twitter for iPhone</td>
      <td>RT @WhiteHouse: LIVE: President @realDonaldTru...</td>
      <td>Mon Aug 03 21:36:46 +0000 2020</td>
      <td>4971</td>
      <td>0</td>
      <td>True</td>
      <td>1290401249251270663</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Twitter for iPhone</td>
      <td>My visits last week to Texas and Florida had m...</td>
      <td>Mon Aug 03 15:27:41 +0000 2020</td>
      <td>24241</td>
      <td>111167</td>
      <td>False</td>
      <td>1290308363872538624</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Twitter for iPhone</td>
      <td>RT @realDonaldTrump: FAKE NEWS IS THE ENEMY OF...</td>
      <td>Mon Aug 03 13:53:10 +0000 2020</td>
      <td>78121</td>
      <td>0</td>
      <td>True</td>
      <td>1290284578578419712</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Twitter for iPhone</td>
      <td>Wow! Really bad TV Ratings for Morning Joe (@J...</td>
      <td>Mon Aug 03 12:57:35 +0000 2020</td>
      <td>15131</td>
      <td>72174</td>
      <td>False</td>
      <td>1290270589945430016</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Twitter for iPhone</td>
      <td>My visits last week to Texas and Frorida had m...</td>
      <td>Mon Aug 03 12:46:02 +0000 2020</td>
      <td>12976</td>
      <td>59426</td>
      <td>False</td>
      <td>1290267685117460481</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4238</th>
      <td>Twitter Web Client</td>
      <td>Donald Trump appearing today on CNN Internatio...</td>
      <td>Wed Feb 10 15:17:56 +0000 2010</td>
      <td>7</td>
      <td>1</td>
      <td>False</td>
      <td>8905123688</td>
    </tr>
    <tr>
      <th>4239</th>
      <td>Twitter Web Client</td>
      <td>Celebrity Apprentice returns to NBC, Sunday, 3...</td>
      <td>Tue Jan 12 18:05:08 +0000 2010</td>
      <td>20</td>
      <td>3</td>
      <td>False</td>
      <td>7677152231</td>
    </tr>
    <tr>
      <th>4240</th>
      <td>Twitter Web Client</td>
      <td>Reminder: The Miss Universe competition will b...</td>
      <td>Sun Aug 23 21:12:37 +0000 2009</td>
      <td>1</td>
      <td>4</td>
      <td>False</td>
      <td>3498743628</td>
    </tr>
    <tr>
      <th>4241</th>
      <td>Twitter Web Client</td>
      <td>Watch the Miss Universe competition LIVE from ...</td>
      <td>Fri Aug 21 14:32:45 +0000 2009</td>
      <td>1</td>
      <td>3</td>
      <td>False</td>
      <td>3450626731</td>
    </tr>
    <tr>
      <th>4242</th>
      <td>Twitter Web Client</td>
      <td>Read a great interview with Donald Trump that ...</td>
      <td>Wed May 20 22:29:47 +0000 2009</td>
      <td>4</td>
      <td>3</td>
      <td>False</td>
      <td>1864367186</td>
    </tr>
  </tbody>
</table>

Zusätzlich werden mit verschiedenen Methoden <strong>Retweets herausgefiltert</strong>, da diese für die Analyse nicht weiter relevant sind (Trumps eigene Äußerungen über Tweets sollen im Vordergrund stehen). 

Ein letzter Vorverarbeitungsschritt betrifft das verwendete Zeitformat - für eine Analyse werden die Daten in das UTC-Format transformiert und einzelne Jahre in ähnlicher Weise daraus extrahiert.

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code># transform col "created_at" into recognizable UTC time values as dates/date type
# see also https://stackoverflow.com/questions/7703865/going-from-twitter-date-to-python-datetime-date

# define function, using datetime module
def change_dates(dataframe, index):
    for tweet in dataframe:
        dataframe.iloc[index] = datetime.strftime(datetime.strptime(dataframe.iloc[index],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')
        index += 1

# call function for col "created_at"
change_dates(df_media["created_at"], 0)
</code></pre></div>

<a class="internal-link" href="#inhalt">↑</a>

<h2><span id="textverarbeitung">Textverarbeitung</span></h2>

Mittels Regular Expressions werden außerdem <strong>Hashtags und Mentions identifiziert</strong>, welche für eine Auswertung in einer separaten Spalte gespeichert werden.

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code># search for hashtags and mentions, put them in own columns

#hashtags
hashtag_re = re.compile("#(\w+)")
df_media['hashtags'] = np.where(df_media.text.str.contains(hashtag_re), df_media.text.str.findall(hashtag_re), "")

#mentions
mention_re = re.compile("@(\w+)")
df_media['mentions'] = np.where(df_media.text.str.contains(mention_re), df_media.text.str.findall(mention_re), "")
</code></pre></div>

Weiter unten soll für die Tweets eine simplifizierte <a class="external-link" href="https://fortext.net/routinen/methoden/sentimentanalyse" target="_blank" rel="noopener noreferrer">Sentimentanalyse</a> durchgeführt werden. Hierzu werden zunächst die Tweets mit der <a class="external-link" href="https://pypi.org/project/tweet-preprocessor/" target="_blank" rel="noopener noreferrer">preprocessor</a>-Library in einem weiteren (Text-)Vorverarbeitungsschritt von Informationen wie Hashtags, Mentions oder URLs bereinigt, sodass diese die Bewertung des Sentimentwerts nicht beeinflussen. 

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>df_media['cleaned_text'] = df_media["text"] #copy col text to cleaned_text

p.set_options(p.OPT.HASHTAG, p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION) #set options for cleaning: clean text from hashtags, mentions, urls and emojis

# define function clean_text() which will append the predefined options to each row
def clean_text(dataframe, index):
    for row in dataframe:
        dataframe.iloc[index] = p.clean(dataframe.iloc[index])
        index += 1

# call clean_text() for col "cleaned_text"
clean_text(df_media["cleaned_text"], 0)</code></pre></div>

Für die Sentimentanalyse wird in folgendem Beispiel die Bibliothek <a class="external-link" href="https://textblob.readthedocs.io/en/dev/" target="_blank" rel="noopener noreferrer">Textblob</a> verwendet. 

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>#create df containing polarity values for each tweet in the dataset

tweet_text = [] # create an empty list

for index, col in df_media.iterrows(): 
    tweet_text.append(df_media.cleaned_text[index]) # append each tweet to it

sentiment_objects = [TextBlob(tweet) for tweet in tweet_text] # apply textblob for each tweet in tweet_text

sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_objects] # create a new list of polarity values and tweet text

sentiment_df = pd.DataFrame(sentiment_values, columns=["polarity", "tweet"]) # transform list into dataframe and sort the values accordingly
sentiment_df.sort_values("polarity", ascending=True)</code></pre></div>

Wir erhalten eine Liste aller Tweets, sortiert nach Polaritätswerten:

<table style="width:100%">
  <thead>
    <tr>
      <th></th>
      <th style="white-space:nowrap">polarity</th>
      <th>tweet</th>
    </tr>
  </thead>
  <tbody style="text-align:center">
    <tr>
      <th style="white-space:nowrap">1756</th>
      <td>-1.0</td>
      <td>find the leakers within the FBI itself. Classi...</td>
    </tr>
    <tr>
      <th>2119</th>
      <td>-1.0</td>
      <td>FMR PRES of Mexico, Vicente Fox horribly used ...</td>
    </tr>
    <tr>
      <th>2620</th>
      <td>-1.0</td>
      <td>If you look at the horrible picture on the fro...</td>
    </tr>
    <tr>
      <th>2908</th>
      <td>-1.0</td>
      <td>The media is pathetic. Our embassies are savag...</td>
    </tr>
    <tr>
      <th>1316</th>
      <td>-1.0</td>
      <td>Some people HATE the fact that I got along wel...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>472</th>
      <td>1.0</td>
      <td>Triggered, a great book by my son, Don. Now nu...</td>
    </tr>
    <tr>
      <th>972</th>
      <td>1.0</td>
      <td>Great news!</td>
    </tr>
    <tr>
      <th>969</th>
      <td>1.0</td>
      <td>Finally great news at the Border!</td>
    </tr>
    <tr>
      <th>1501</th>
      <td>1.0</td>
      <td>Great news, as a result of our TAX CUTS &amp;amp; ...</td>
    </tr>
    <tr>
      <th>696</th>
      <td>1.0</td>
      <td>Jesse &amp;amp; Emma, Great News. Congratulations!...</td>
    </tr>
  </tbody>
</table>

Um die Polarität mit anderen Werten aus unserem Dataframe vergleichen/analysieren zu können, muss dafür eine zusätzliche Spalte hinzugefügt werden.

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>df_media = df_media.reset_index(drop=True)
df_media['sentiment_values'] = pd.Series(sentiment_df['polarity'])</code></pre></div>

Alternativ können die reinen Polaritätswerte auch in Beschreibungen umgewandelt werden, es müssen lediglich entsprechende Grenzwerte gefunden werden (in diesem Codebeispiel: negativ&lt;= -0.33 &lt; neutral &lt;= 0.33 &lt; positiv).

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>df_media["sentiment"] = "" # new col

# define function to devide sentiment_values into sentiments ("negative","neutral","positive")
def get_sentiment(dataframe, index):
    for tweet in dataframe:
        if df_media.sentiment_values.iloc[index] <= -0.33:
            df_media.sentiment.iloc[index] = "negative"
        elif (df_media.sentiment_values.iloc[index] > -0.33) & (df_media.sentiment_values.iloc[index] <=0.33):
            df_media.sentiment.iloc[index] = "neutral"
        else:
            df_media.sentiment.iloc[index] = "positive"
        index += 1

# apply get_sentiment()
get_sentiment(df_media["sentiment_values"], 0)</code></pre></div>

<a class="internal-link" href="#inhalt">↑</a>

<h2><span id="analyse">Datenanalyse</span></h2>
<h3><span id="deskriptive-analyse">Deskriptive Analyse</span></h3>

Mit der Funktion <span class="tag-code">pandas.Dataframe.describe()</span> können erste Kennzahlen für Minimum, Maximum, Mittelwert, Standardabweichung und Quantile generiert werden. Desweiteren kann die Verteilung der Ausprägungen beispielsweise mittels Histogramm oder Boxplot visualisiert werden. 

<img src="../img/tutorial_tweets-analysieren-mit-python/hist_retweet_favorite.png" height=100% width=100%/>

<img src="../img/tutorial_tweets-analysieren-mit-python/hist_sentiment.png" height=100% width=100%/>

Die oben gezeigten <strong>Histogramme</strong> deuten bereits eine rechtsschiefe Verteilung für Retweets und Likes im Datensatz an. Für die im Nachhinein berechneten Polaritätswerte scheint annäherungsweise eine Normalverteilung vorzuliegen. Nachfolgend wird beispielhaft gezeigt, wie die Erstellung eines solchen Histogramms mit <span class="tag-code">matplotlib.pyplot.hist()</span> aussehen könnte.

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code># plot data
fig, ax = plt.subplots()
ax.hist(df_media.retweet_count, color = "blue", alpha=0.5, bins=20)
ax.hist(df_media.favorite_count, color = "aquamarine", alpha=0.5, bins=60)

# add labels, title, legend
ax.set_ylabel('Häufigkeit')
ax.set_xlabel('Anzahl der Retweets/Likes')
ax.set_title(r'Verteilungen retweet_count/favorite_count')
ax.legend()

#show plot
plt.show()
</code></pre></div>

<img src="../img/tutorial_tweets-analysieren-mit-python/boxplots.jpg" height=100% width=100%/>

<strong>Boxplots</strong> sind eine weitere Möglichkeit zur Visualisierung der Verteilung numerischer Variablen. Die Abbildung zeigt Beispiele für die gleichen drei Variablen, erstellt mit <span class="tag-code">plotly.graph_objects.Box</span>. 

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code># boxplot favorite_count
data = df_media.favorite_count

# plot
fig = go.Figure()
fig.add_trace(go.Box(y=data, name="favorite_count", marker_color="lightseagreen", boxpoints="all", jitter=0.6,
    pointpos=-2, marker=dict(opacity=0.5), marker_size=5, width=40))

# update layout
fig.update_layout(width=400, height=800, font=dict(color="black"), plot_bgcolor="white", xaxis = dict(gridcolor = "#ededed", gridwidth=3), yaxis=dict(gridcolor = "#ededed", gridwidth=3))

#show plot
fig.show()</code></pre></div>

Die bisherigen Visualisierungen sind auf metrische Variablen ausgerichtet, für nominal bzw. ordinal skalierte Daten muss ggf. auf alternative, simplere Methoden zurückgegriffen werden.

<img src="../img/tutorial_tweets-analysieren-mit-python/pie_hashtags.png" height=100% width=100%/>

Dieses <strong>Sunburst Chart</strong> (Spezialfall eines Pie Charts) beinhaltet die in den Tweets des Datensatzes enthaltenen Hashtags, welche mit ein wenig zusätzlichem qualitativen Aufwand in allgemeinere Kategorien ("medien", "medienkritik", "politisches", "show", "sonstiges", "wahlkampf") zusammengefasst wurden. Neben den zu erwartenden Hashtags zu Medien ("medien", "medienkritik", "show") sind insbesondere viele Hashtags enthalten, die der Kategorie "wahlkampf" zugeordnet wurden (u.a. #maga, #trump2016). 

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code># plot
fig = px.sunburst(hashtags_df, path=["category", "hashtag"], values='frequency', color="frequency", color_continuous_scale=[[0,"white"],[1.0, "blue"]])

# update layout
fig.update_layout(width=900, height=900, uniformtext=dict(minsize=13, mode='hide'),coloraxis_showscale=False) # labels of hashtags with frequency=1 are not shown (by "minsize=13, mode='hide'")

# show plot
fig.show()
</code></pre></div>

<img src="../img/tutorial_tweets-analysieren-mit-python/bar_mentions_top10.png" height=100% width=100%/>

Bei dem <strong>Bar Chart</strong> handelt es sich um eine Darstellung der 10 am häufigsten erwähnten Accounts (<span class="tag-code">mentions</span>). Primär besteht diese aus Accounts der Nachrichtensender, die bereits als Begriffe für die Zusammenstellung des Datensatzes verwendet wurden (@cnn, @nytimes, @foxnews, @nbc, @foxandfriends, @abc, @nbcnews sowie zusätzlich die @washingtonpost).

Hierfür werden (bei den Hashtags ist dieser Schritt ähnlich) zuerst die Mentions in eine Liste überführt, dann die einzelnen Häufigkeiten dieser Mentions ausgezählt und die Ergebnisse in einem neuem Dataframe gespeichert sowie nach der Häufigkeit sortiert. 

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>mentions = [] # create empty list

# iterate through all mentions and append them to list
for index, col in df_media.iterrows():
    mentions.append(df_media.mentions[index])

# change nested list to flat list
# see https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists

flat_list = []
for sublist in mentions:
    for item in sublist:
        item = item.lower()
        flat_list.append(item)


# create dict for mentions by counting occurences of each item, then transform mentions and frequencies into a dataframe (mentions_df)
mentions_dict = Counter(flat_list)
mentions_df = pd.DataFrame(list(mentions_dict.items()),columns = ["mention", "frequency"]) 

# sort mentions in descending order 
mentions_df = mentions_df.sort_values(by="frequency", ascending=False).reset_index(drop=True)
</code></pre></div>

Anschließend können die ersten zehn Mentions entsprechend visualisiert werden:

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>data = mentions_df.iloc[:11]

# plot
fig = px.bar(data, x="mention", y="frequency")
fig.update_traces(marker_color='lightseagreen', opacity=.6, texttemplate='%{y}', textposition='outside')

# update layout
fig.update_layout(width=800, height=500, font=dict(size=12, color="black"), plot_bgcolor="white", xaxis = dict(gridcolor = "#ededed", gridwidth=1), yaxis=dict(gridcolor = "#ededed", gridwidth=1))

# show plot
fig.show()
</code></pre></div>

<a class="internal-link" href="#inhalt">↑</a>

<h3><span id="relationen">Relationen zwischen Variablen</span></h3>

Zusammenhänge zwischen Variablen lassen sich unter anderem gut in Form von sogenannten <strong>Scatterplots</strong> (Streudiagrammen) darstellen. Für die Visualisierungen wurde weiterhin die Bibliothek <a class="external-link" href="https://plotly.com/python/" target="_blank" rel="noopener noreferrer">Plotly</a> verwendet. Die Berechnung der statistischen Werte wurde mit der Python-Bibliothek <a class="external-link" href="https://researchpy.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">researchpy</a> durchgeführt. Diese macht übersichtliche statistische Zusammenfassungen verfügbar und baut u.a. auf statistische Funktionen von <a class="external-link" href="https://docs.scipy.org/doc/scipy/reference/stats.html" target="_blank" rel="noopener noreferrer">scipy.stats</a> auf.

<img src="../img/tutorial_tweets-analysieren-mit-python/scatter_created-source.png" height=100% width=100%/>

Die Abbildung zeigt die Variablen <span class="tag-code">source</span> (verwendetes Endgerät/Applikation) und <span class="tag-code">created_at</span> (Veröffentlichungszeitpunkt des Tweets) in Form eines solchen Scatterplots. Farblich werden hier wie auch in den nachfolgenden Streudiagrammen die Polaritätswerte der Tweets repräsentiert. Die drei meistgenutzten Kanäle (Twitter for Android, Twitter Web Client und Twitter for iPhone) unterscheiden sich - wie in dieser Darstellung deutlich wird - vor allem nach ihrem zeitlichen Einsatz. Diese zeitliche Unterteilung nach verwendeten "tweet sources" deckt sich u.a. mit den Beobachtungen von [<a class="internal-link" href="#clarke-grieve-2019">Clarke/Grieve 2019</a>, S. 5f.] und deren deutlich umfangreicheren Daten.

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>df = df_media

# plot
fig = px.scatter(df, x="created_at", y="source",color="sentiment_values", color_continuous_scale=["orangered","greenyellow","blue"], opacity=.5)

# update layout
fig.update_layout(font=dict(size=12, color="black"), plot_bgcolor="white", xaxis = dict(gridcolor = "#ededed", gridwidth=2), yaxis=dict(gridcolor = "#ededed", gridwidth=2), width=1200)

# show plot
fig.show()
</code></pre></div>

Da das Merkmal <span class="tag-code">source</span> nominalskaliert ist, wird der <strong>Chi-Quadrat-Test</strong> angewendet. Hierbei sollte darauf geachtet werden, dass die erwarteten Häufigkeiten jeweils über 5 liegen, in jedem Fall jedoch "der Anteil der erwarteten Häufigkeiten, die kleiner als 5 sind, 20% nicht überschreite[n]" [<a class="internal-link" href="#bortz-2005">Bortz 2005</a>, S. 177] darf. Im vorliegenden Fall wurde der Datensatz um die selten auftretenden Merkmalsausprägungen (alle "sources" bis auf Twitter Web Client, Twitter for Android und Twitter for iPhone; das Jahr 2009) reduziert, um diese Voraussetzung zu erfüllen. 

<table style="width:100%">
  <thead>
    <tr>
      <th>Datensatz</th>
      <th style="white-space:nowrap">Pearson Chi-Quadrat</th>
      <th>p-Wert</th>
      <th>Cramers V</th>
    </tr>
  </thead>
  <tbody style="text-align:center">
    <tr>
      <th>ursprünglich (df=143.0)</th>
      <td>4984.11</td>
      <td>0.00</td>
      <td>0.38</td>
    </tr>
    <tr>
      <th>angepasst (df=20.0)</th>
      <td>3236.34</td>
      <td>0.00</td>
      <td>0.73</td>
    </tr>
</tbody>
</table>

Für die angepassten Daten (df=20) zeichnet sich ein starker Effekt (V = 0.73) ab, wodurch davon ausgegangen werden kann, dass zumindest zwischen den am häufigsten verwendeten Endgeräten und den zeitliche Abständen in Jahren ein gewisser Zusammenhang im Datensatz gefunden werden kann. Der Chi-Quadrat-Test wurde mithilfe der Funktion <span class="tag-code">crosstab()</span> durchgeführt.

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>crosstab, res, expected = rp.crosstab(df_media.source, df_media.year,prop="cell",test="chi-square",correction=True, cramer_correction=True, expected_freqs=True)
</code></pre></div>
<img src="../img/tutorial_tweets-analysieren-mit-python/scatter_created-retweet.png" height=100% width=100%/>

<img src="../img/tutorial_tweets-analysieren-mit-python/scatter_created-favorite.png" height=100% width=100%/>

Die Scatterplots zeigen die Verteilung von Retweets bzw. Likes über die Zeit, wobei diese auf den ersten Blick sehr ähnlich ausfallen (auch wenn die Werte für <span class="tag-code">favorite_count</span> nach oben stärker gestreckt sind). Auffällig ist vor allem der starke Anstieg sowohl von Retweets als auch Likes für Tweets ab ca. 2016, also zum Zeitpunkt von Trumps Präsidentschaftskandidatur. Die folgende Tabelle fasst die Werte für die Korrelation mit <strong>parametrischen (Pearson) wie nicht-parametrischen Verfahren (Spearman, Kendall)</strong> für die Variablen <span class="tag-code">retweet_count</span> und <span class="tag-code">year</span>, <span class="tag-code">favorite_count</span> und <span class="tag-code">year</span> sowie <span class="tag-code">favorite_count</span> und <span class="tag-code">retweet_count</span> zusammen.


<table style="width:100%">
  <thead>
    <tr>
      <th>Variablen</th>
      <th>Methode</th>
      <th>r-Wert</th>
      <th>p-Wert</th>
    </tr>
  </thead>
  <tbody style="text-align:center">
    <tr>
      <th rowspan="3">retweet_count x year</th>
      <td>Pearson</td>
      <td>0.69</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>Spearman</td>
      <td>0.75</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>Kendall</td>
      <td>0.59</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th rowspan="3">favorite_count x year</th>
      <td>Pearson</td>
      <td>0.71</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>Spearman</td>
      <td>0.78</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>Kendall</td>
      <td>0.64</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th rowspan="3">favorite_count x retweet_count</th>
      <td>Pearson</td>
      <td>0.96</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>Spearman</td>
      <td>0.98</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>Kendall</td>
      <td>0.88</td>
      <td>0.00</td>
    </tr>
</tbody>
</table>

Die Korrelationskoeffizienten können mit <span class="tag-code">researchpy.corr_pair()</span> berechnet werden:

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code>rp.corr_pair(df_media[["retweet_count","favorite_count"]], method="kendall")
</code></pre></div>

Die in der Tabelle festgehaltenen Werte für die Korrelation von <span class="tag-code">retweet_count</span> x <span class="tag-code">year</span> und <span class="tag-code">favorite_count</span> x <span class="tag-code">year</span> deuten auf einen starken und positiven linearen Zusammenhang hin. Die Variablen <span class="tag-code">retweet_count</span> und <span class="tag-code">favorite_count</span> mit ihren übermäßig hohen Werten für einen Zusammenhang könnten allerdings ein Anzeichen dafür sein, dass es sich um eine <strong>Scheinkorrelation</strong> ("spurious relationship"/"spurious correlation") handelt: 

<blockquote><span class="citation">A relationship between two variables that is caused by a statistical artifact or a factor, not included in the model, that is related to both variables.</span> (<a class="external-link" href="#downey-2014">Downey 2014</a>, S. 143)</blockquote> 

Es könnte demnach eine Korrelation bestehen, die jedoch nicht kausal begründet werden kann. Vermutlich ist eine weitere, sogenannte <strong>konfundierende Variable</strong> ("Störvariable"), wie beispielsweise die über die Zeit ansteigende Followerzahl, ursächlich für eine ähnlich hohe Anzahl an Retweets wie Likes [vgl. <a class="internal-link" href="wang-2016">Wang et al. 2016</a>, S. 721]. <a class="internal-link" href="#inhalt">↑</a>

<h3><span id="weiterführende-analyse">Weiterführende Analyse</span></h3>

Die hier vorgestellte Auswahl an untersuchten Relationen zwischen Variablen lässt die Annahme zu, dass ein gewisser Zusammenhang zwischen verschiedenen Variablen und der zeitlichen Komponente, z.B. durch zunehmende Followerzahlen über die Zeit oder stategische Anpassungen im Laufe der Zeit, besteht. Die zeitliche Abhängigkeit linguististischer Veränderungen in Tweets Trumps wurde bereits von [<a class="internal-link" href="#clarke-grieve-2019">Clarke/Grieve 2019</a>] im Detail beleuchtet. Weiterführend könnte insofern beispielsweise analysiert werden, welche Inhalte Trump auf Twitter veröffentlicht und wie die Twitter-Community wiederum auf diese reagiert. Eine Forschungsfrage könnte demnach sein: 

<em>Was sind die häufigsten Aussagen (gemessen in N-Grammen) im Datensatz und wie sind diese über die Zeit verteilt?</em>

Um diese Fragestellung zu adressieren, müssen zunächst alle Tweets in sogenannte <strong>N-Gramme</strong>, hier 5-Gramme (auch: Pentagramme) zerlegt werden. Dafür wurde das Modul <span class="tag-code">ngrams</span> des <a class="external-link" href="https://www.nltk.org/" target="_blank" rel="noopener noreferrer">NLTK</a> (Natural Language Toolkit) verwendet.

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code># copy df for this step, it gets messy
# df_media should stay as it is (deep=True)
df_ngrams = df_media.copy(deep=True)

#create a new column "ngrams"
df_ngrams["ngrams"] = ""

#def function to iterate through df and get all 5-grams within "cleaned_text"
def get_ngrams(dataframe, index):
    for tweet in dataframe:
        list_ngrams = []
        n = 5
        fivegrams = list(ngrams(df_ngrams.cleaned_text.iloc[index].split(), n))
        dataframe.iloc[index] = fivegrams
        index += 1

# call get_ngrams()
get_ngrams(df_ngrams["ngrams"], 0)
</code></pre></div>

Um die N-Gramme tatsächlich auswerten zu können, müssen diese aus den verschachtelten Listen getrennt werden - ein Aufsplitten mit der <span class="tag-code">pandas.DataFrame.explode()</span>-Funktion löst das Problem jedoch nicht, da hier alle enthaltenen Listen aufgetrennt und anschließend nur noch einzelne Wörter enthalten sind. Eine Liste nur auf der obersten Ebene aufzusplitten gelingt über eine Nachnutzung der Funktion <span class="tag-code">tidy_split()</span> (siehe <a class="external-link" href="https://github.com/cognoma/genes/blob/721204091a96e55de6dcad165d6d8265e67e2a48/2.process.py#L61-L95" target="_blank" rel="noopener noreferrer">Github</a>):

<div class="hcb_wrap">
<pre class="prism undefined-numbers lang-python" data-lang="Python"><code># call function tidy_split() and separate all ngrams per tweet from each other
df_grams = tidy_split(df_ngrams,"ngrams",sep="), (")
</code></pre></div>

In der folgenden Tabelle sind die <strong>10 häufigsten Pentagramme</strong> aus den Tweets einzusehen.

<table style="width:100%">
  <thead>
    <tr>
      <th>Pentagramm</th>
      <th>Anzahl</th>
    </tr>
  </thead>
  <tbody style="text-align:center">
    <tr>
      <td>'the', 'failing', 'new', 'york', 'times'</td>
      <td>30</td>
    </tr>
    <tr>
      <td>'the', 'enemy', 'of', 'the', 'people!'</td>
      <td>25</td>
    </tr>
    <tr>
      <td>'the', 'fake', 'news', 'media', 'is'</td>
      <td>19</td>
    </tr>
    <tr>
      <td>'be', 'on', 'fox', '&amp;', 'friends'</td>
      <td>15</td>
    </tr>
    <tr>
      <td>'will', 'be', 'on', 'fox', '&amp;'</td>
      <td>15</td>
    </tr>
    <tr>
      <td>'i', 'will', 'be', 'having', 'a'</td>
      <td>13</td>
    </tr>
    <tr>
      <td>'be', 'doing', 'fox', '&amp;', 'friends'</td>
      <td>12</td>
    </tr>
    <tr>
      <td>'is', 'the', 'enemy', 'of', 'the'</td>
      <td>12</td>
    </tr>
    <tr>
      <td>'the', 'history', 'of', 'our', 'country.'</td>
      <td>11</td>
    </tr>
</tbody>
</table>

<img src="../img/tutorial_tweets-analysieren-mit-python/scatter_ngrams-created.png" height=100% width=100%/>

Betrachtet man diese häufigsten Textfragmente im Datensatz nach ihrer zeitlichen Veröffentlichung, so wird deutlich, was aufgrund der vorausgegangen Untersuchungen schon erahnt werden konnte: spätestens ab dem Amtsantritt Trumps scheinen sich auch Sprache und Inhalte der Tweets zu ändern. Die Ergebnisse von Clarke/Grieve (2019) stimmen damit überein:

<blockquote><span class="citation">All four dimensions showed clear temporal patterns and most major shifts in style align to a small number of indisputably important points in the Trump timeline, especially the 2011 Birther controversy, the 2012 election, his 2015 declaration, his 2016 Republican nomination, the 2016 election, and his 2017 inauguration, as well as the seasons of his television series <em>The Apprentice</em>.</span> (<a class="external-link" href="#clarke-grieve-2019">Clarke/Grieve 2019</a>, S. 19)</blockquote>

Es sei zwar darauf hingewiesen, dass in der Abbildung nur ein kleiner Ausschnitt aller N-Gramme dargestellt wird (es wurden insgesamt 75756 N-Gramme aus dem Datensatz extrahiert). Dennoch kann vermutet werden, dass sich frühere Tweets eher mit Medien im Kontext des Unterhaltungsfernsehens (hier: Fox & Friends) auseinandersetzen, während insbesondere ab 2017 die Anzahl der distanzierenden und beschuldigenden Aussagen in Richtung Medien (Stichwörter wären "enemy", "fake news media", "failing") offensichtlich zunimmt. So macht Trump beispielsweise "die Fake News" oder "Fake News Medien" zum Feind ("the enemy of the people"):

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">CNN and others in the Fake News Business keep purposely and inaccurately reporting that I said the “Media is the Enemy of the People.” Wrong! I said that the “Fake News (Media) is the Enemy of the People,” a very big difference. When you give out false information - not good!</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href="https://twitter.com/realDonaldTrump/status/1057059603605831680?ref_src=twsrc%5Etfw">October 30, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

Natürlich sind die hier angeführten Beobachtungen allein nicht Beweis genug und es müssten weitere Analysen durchgeführt werden, um entsprechende Annahmen zu festigen. <a class="internal-link" href="#inhalt">↑</a>

<h2><span id="ausblick">Ausblick</span></h2>

Abschließend sollen einige wesentliche Erkenntnisse zusammengefasst werden.

<strong>1. Trump verfolgt auf Twitter eine klare (Kommunikations-)Strategie.</strong> Dies lässt sich u.a. durch die zeitliche Abhängigkeit verschiedener Aspekte beschreiben - beispielsweise der verwendeten Endgeräte/Applikationen für die Veröffentlichung von Tweets sowie der divergierenden Inhalte der Tweets. Generell lässt sich ab 2016 ein deutlicher Anstieg der Tweets beobachten. Wichtige Ereignisse innerhalb der betrachteten Zeitspanne haben vermutlich einen Einfluss auf die Inhalte und den Stil der Tweet, und könnten als Gegenstand späterer Untersuchungen dienen. [<a class="internal-link" href="clarke-grieve-2019">Clarke/Grieve 2019</a>]

<strong>2. Die Medienkritik Trumps (auf Twitter) ist stark politisch geprägt und wird als Instrument für den Wahlkampf eingesetzt.</strong> Aus den Tweets extrahierte Hashtags und Begriffe beinhalten neben kritischen, häufig attackierenden Äußerungen gegen Medien und Nachrichten z.B. den Wahlslogan #makeamericagreatagain / #maga und #trump2016, später #kag ("Keep America Great"). Eine Wiederholung entsprechender Phrasen wie "'fake news', 'failing' und 'enemy of the American People'"[frei übersetzt nach <a class="internal-link" href="#meeks-2019">Meeks 2019</a>, S. 5] sorgt zudem für ein Framing bestimmter Medien bzw. des gesamten Mediensystems, welches auf besonders große Resonanz trifft.

<strong>3. Das hybride Mediensystem wurde von Trump für eine möglichst breite mediale Abdeckung ausgenutzt.</strong> Das Teilen von Informationen in sozialen Medien wie Twitter sorgt einerseits dafür, dass User ihre Zustimmung zu einer bestimmten Information bzw. Meinung geben können, andererseits werden hierdurch auch Informationen in ideologisch orientierten Communities, aber auch darüber hinaus weitergetragen - was schließlich wiederum die Aufmerksamkeit traditioneller Nachrichtenmedien erregen kann. [<a class="internal-link" href="#wells-2020">Wells et al. 2020</a>, S. 664f.]

<blockquote><span class="citation">Furthermore, Trump tweeted more at times when he had recently garnered less of a relative advantage in news attention, suggesting he strategically used Twitter to trigger coverage.</span> (<a class="external-link" href="#wells-2020">Wells et al. 2020</a>, S. 559)</blockquote>

Demnach greift Trump mit seinen Tweets nicht nur die Opposition an, sondern attackiert ebenso für ihn gefährliche Medien [<a class="internal-link" href="#clarke-grieve-2019">Clarke/Grieve 2019</a>, S. 20] [<a class="internal-link" href="#wang-2016">Wang et al. 2016</a>, S. 719][<a class="internal-link" href="#wells-2020">Wells et al. 2020</a>, S. 661], indem er mittels Framing ein bereits bestehendes Missvertrauen der Bevölkerung gegenüber den Medien verstärkt [<a class="internal-link" href="#meeks-2019">Meeks 2019</a>, S. 5]. Letztlich könnte sich das von Trump geschaffene und über Twitter weiter verbreitete Bild der Medien auch auf die generelle öffentliche Wahrnehmung der medialen Welt auswirken [<a class="internal-link" href="#meeks-2019">Meeks 2019</a>, S. 19]. <a class="internal-link" href="#inhalt">↑</a>

<h2>Literaturangaben</h2>

<p id="bortz-2005"><strong>Bortz, Jürgen (2005)</strong>: Statistik für Human- und Sozialwissenschaftler, 6. Aufl., Berlin/Heidelberg: Springer.</p>

<p id="clarke-grieve-2019"><strong>Clarke, Isobelle & Grieve, Jack (2019)</strong>: Stylistic variation on the Donald Trump Twitter account: A linguistic analysis of tweets posted between 2009 and 2018, in: PLoS One 14(9), S. 1-27. <a class="external-link" href="https://doi.org/10.1371/journal.pone.0222062" target="_blank" rel="noopener noreferrer">https://doi.org/10.1371/journal.pone.0222062</a></p>

<p id="downey-2014"><strong>Downey, Allen B. (2014)</strong>: Think Stats. Exploratory Data Analysis, 2. Aufl., O'Reilly Media, Inc.</p>

<p id="entman-1993"><strong>Entman, Robert M. (1993)</strong>: Framing: Toward clarification of a fractured paradigm, in Journal of Communication 43(4), S. 51-58. <a class="external-link" href="https://doi.org/10.1111/j.1460-2466.1993.tb01304.x" target="_blank" rel="noopener noreferrer">https://doi.org/10.1111/j.1460-2466.1993.tb01304.x</a></p>

<p id="meeks-2019"><strong>Meeks, Lindsey (2019)</strong>: Defining the Enemy: How Donald Trump Frames the News Media, in Journalism & Mass Communication Quarterly 97(1), S. 211-234. <a class="external-link" href="https://doi.org/10.1177/1077699019857676" target="_blank" rel="noopener noreferrer">https://doi.org/10.1177/1077699019857676</a></p>

<p id="wang-2016"><strong>Wang, Yu; Luo, Jiebo; Niemi, Richard; Li, Yuncheng & Hu, Tianran (2016)</strong>: Catching Fire via "Likes": Inferring Topic Preferences of Trump Followers on Twitter, in Proceedings of the Tenth International AAAI Conference on Web and Social Media (ICWSM 2016), S. 719-722.</p>

<p id="wells-2020"><strong>Wells, Chris; Shav, Dhavan; Lukito, Josephine; Pelled, Ayellet; Pevehouse, Jon CW & Yang, Jung Hwan (2020)</strong>: Trump, Twitter, and news media responsiveness: A media system approach, in new media & society 22(4), S. 659-682. <a class="external-link" href="https://doi.org/10.1177/1461444819893987" target="_blank" rel="noopener noreferrer">https://doi.org/10.1177/1461444819893987</a></p>

</div>
<script type="text/javascript" src="../functions/functions_all.js"></script>
</body>
</html>
